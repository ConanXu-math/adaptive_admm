{"id": "39761723-1885-406e-9302-ec27f21ef134", "code": "# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.0, p=1.2):\n    \"\"\"\n    Diminishing step size, must be summable.\n    AlphaEvolve is allowed to change this.\n    \"\"\"\n    return c / ((k + 1) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=3.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    Adaptive ADMM penalty update rule.\n\n    Inputs:\n        rho    : current penalty parameter\n        k      : iteration index\n        r_norm : ||primal residual||\n        s_norm : ||dual residual||\n    Outputs:\n        new_rho : updated rho\n        aux     : any auxiliary scalar (logged but not used)\n        mode    : string label (\"mul\", \"div\", \"keep\", etc.)\n\n    AlphaEvolve may change the internal logic,\n    but MUST respect the signature and return types.\n    \"\"\"\n\n    t = tau(k, c, p)\n    # Compute the ratio of residuals, avoiding division by zero\n    ratio = (r_norm + eps) / (s_norm + eps)\n    \n    # Target ratio is 1, so we want ratio to be close to 1\n    # If ratio > mu, primal residual is too large -> increase rho\n    # If ratio < 1/mu, dual residual is too large -> decrease rho\n    # Use a smoother update factor based on sqrt(ratio) dampened by t\n    # Cap the maximum adjustment to avoid extreme changes\n    if ratio > mu:\n        # Increase rho by factor proportional to sqrt(ratio)\n        # But dampen with t to avoid large jumps in later iterations\n        adj = np.sqrt(ratio)\n        # Bound adjustment between 1.0 and 2.0\n        adj = min(adj, 2.0)\n        fac = 1.0 + t * (adj - 1.0)\n        return rho * fac, t, \"mul\"\n    elif ratio < 1.0 / mu:\n        # Decrease rho\n        adj = np.sqrt(1.0 / ratio)\n        adj = min(adj, 2.0)\n        fac = 1.0 + t * (adj - 1.0)\n        return rho / fac, t, \"div\"\n    else:\n        return rho, t, \"keep\"\n", "language": "python", "parent_id": "e3e7b7e1-1592-4d78-8c9c-6a76f7a88c15", "generation": 1, "timestamp": 1768219112.8993652, "iteration_found": 1, "metrics": {"combined_score": 0.0766992980527246, "metrics": {"converged": true, "iters": 13, "combined_score": 0.0766992980527246}, "artifacts": {"status": "CONVERGED", "iterations": 13, "eval_time": "0.007s"}}, "complexity": 0.0, "diversity": 0.0, "metadata": {"changes": "Change 1: Replace 10 lines with 19 lines\nChange 2: Replace 19 lines with 25 lines", "parent_metrics": {"combined_score": 0.043007227611818806, "metrics": {"converged": true, "iters": 23, "combined_score": 0.043007227611818806}, "artifacts": {"status": "CONVERGED", "iterations": 23, "eval_time": "0.021s"}}, "island": 0}, "prompts": {"diff_user": {"system": "You are an expert in numerical optimization and ADMM algorithms.\n\nYour task is to improve the function update_rho(rho, k, r_norm, s_norm, ...)\nused to adaptively update the ADMM penalty parameter.\n\nGoals:\n- Achieve faster convergence (fewer ADMM iterations)\n- Maintain numerical stability (avoid oscillating rho)\n- Balance primal and dual residuals efficiently\n\nConstraints:\n- You may ONLY modify the body of update_rho and helper functions in initial_program.py\n- The function signature and return types must remain unchanged\n- The code must be deterministic and numerically safe\n\nHints:\n- Consider using ratios or log-ratios of r_norm and s_norm\n- Early iterations may benefit from aggressive updates\n- Later iterations should favor stability\n- Piecewise or stateful strategies are allowed\n", "user": "# Current Program Information\n- Fitness: 0.0430\n- Feature coordinates: \n- Focus areas: - Fitness unchanged at 0.0430\n- No feature coordinates\n- Consider simplifying - code length exceeds 500 characters\n\n\n\n# Program Evolution History\n## Previous Attempts\n\n### Attempt 1\n- Changes: Unknown changes\n- Metrics: combined_score: 0.0430, metrics: {'converged': True, 'iters': 23, 'combined_score': 0.043007227611818806}, artifacts: {'status': 'CONVERGED', 'iterations': 23, 'eval_time': '0.021s'}\n- Outcome: Improvement in all metrics\n\n## Top Performing Programs\n\n### Program 1 (Score: 0.0430)\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.0, p=1.2):\n    \"\"\"\n    Diminishing step size, must be summable.\n    AlphaEvolve is allowed to change this.\n    \"\"\"\n    return c / ((k + 1) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=3.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    Adaptive ADMM penalty update rule.\n\n    Inputs:\n        rho    : current penalty parameter\n        k      : iteration index\n        r_norm : ||primal residual||\n        s_norm : ||dual residual||\n    Outputs:\n        new_rho : updated rho\n        aux     : any auxiliary scalar (logged but not used)\n        mode    : string label (\"mul\", \"div\", \"keep\", etc.)\n\n    AlphaEvolve may change the internal logic,\n    but MUST respect the signature and return types.\n    \"\"\"\n\n    t = tau(k, c, p)\n    fac = 0.85 + t\n\n    if r_norm > mu * max(s_norm, eps):\n        return rho * fac, t, \"mul\"\n\n    if s_norm > mu * max(r_norm, eps):\n        return rho / fac, t, \"div\"\n\n    return rho, t, \"keep\"\n\n```\nKey features: Performs well on combined_score (0.0430), Performs well on metrics ({'converged': True, 'iters': 23, 'combined_score': 0.043007227611818806}), Performs well on artifacts ({'status': 'CONVERGED', 'iterations': 23, 'eval_time': '0.021s'})\n\n\n\n# Current Program\n```python\n# initial_program.py\n# This file is evolved by AlphaEvolve\n# Only update_rho (and helpers) should be modified\n\nimport numpy as np\n\n\ndef tau(k, c=1.0, p=1.2):\n    \"\"\"\n    Diminishing step size, must be summable.\n    AlphaEvolve is allowed to change this.\n    \"\"\"\n    return c / ((k + 1) ** p)\n\n\ndef update_rho(\n    rho,\n    k,\n    r_norm,\n    s_norm,\n    mu=3.0,\n    c=1.0,\n    p=1.2,\n    eps=1e-12,\n):\n    \"\"\"\n    Adaptive ADMM penalty update rule.\n\n    Inputs:\n        rho    : current penalty parameter\n        k      : iteration index\n        r_norm : ||primal residual||\n        s_norm : ||dual residual||\n    Outputs:\n        new_rho : updated rho\n        aux     : any auxiliary scalar (logged but not used)\n        mode    : string label (\"mul\", \"div\", \"keep\", etc.)\n\n    AlphaEvolve may change the internal logic,\n    but MUST respect the signature and return types.\n    \"\"\"\n\n    t = tau(k, c, p)\n    fac = 0.85 + t\n\n    if r_norm > mu * max(s_norm, eps):\n        return rho * fac, t, \"mul\"\n\n    if s_norm > mu * max(r_norm, eps):\n        return rho / fac, t, \"div\"\n\n    return rho, t, \"keep\"\n\n```\n\n# Task\nSuggest improvements to the program that will improve its FITNESS SCORE.\nThe system maintains diversity across these dimensions: complexity, diversity\nDifferent solutions with similar fitness but different features are valuable.\n\nYou MUST use the exact SEARCH/REPLACE diff format shown below to indicate changes:\n\n<<<<<<< SEARCH\n# Original code to find and replace (must match exactly)\n=======\n# New replacement code\n>>>>>>> REPLACE\n\nExample of valid diff format:\n<<<<<<< SEARCH\nfor i in range(m):\n    for j in range(p):\n        for k in range(n):\n            C[i, j] += A[i, k] * B[k, j]\n=======\n# Reorder loops for better memory access pattern\nfor i in range(m):\n    for k in range(n):\n        for j in range(p):\n            C[i, j] += A[i, k] * B[k, j]\n>>>>>>> REPLACE\n\nYou can suggest multiple changes. Each SEARCH section must exactly match code in the current program.\nBe thoughtful about your changes and explain your reasoning thoroughly.\n\nIMPORTANT: Do not rewrite the entire program - focus on targeted improvements.", "responses": ["Looking at the current implementation, the update strategy uses a fixed factor `fac = 0.85 + t` where `t` diminishes with iterations. This can be improved by making the update more adaptive to the ratio of residuals and by adjusting the aggressiveness based on iteration count.\n\nThe current approach updates rho by multiplying or dividing by `fac`, which starts at `0.85 + 1.0/(1^1.2) \u2248 1.85` and gradually approaches 0.85. This might be too aggressive early on and too conservative later. We want to balance primal and dual residuals more effectively.\n\nKey ideas:\n1. Use the ratio `r_norm / s_norm` to guide updates more precisely.\n2. Make updates more aggressive in early iterations and more stable later.\n3. Avoid oscillating rho by using a smoother adjustment.\n\nLet's modify the update_rho function to:\n- Compute the ratio of residuals, but handle cases where either is near zero.\n- Use a target ratio of 1 (balanced residuals).\n- Adjust rho by a factor proportional to the square root of the ratio, which is a common heuristic.\n- Incorporate iteration-dependent scaling to reduce adjustments over time.\n\nHere's the plan:\n1. Compute `ratio = (r_norm + eps) / (s_norm + eps)`\n2. If ratio is too large (primal residual dominates), increase rho.\n3. If ratio is too small (dual residual dominates), decrease rho.\n4. Adjust the update factor to be `exp(t * log(ratio) / 2)` which is equivalent to `sqrt(ratio)^t`.\n5. Use `t` to dampen updates as iterations progress.\n\nHowever, to maintain compatibility, we'll keep the same return types and structure.\n\nLet's implement:\n\n<<<<<<< SEARCH\n    t = tau(k, c, p)\n    fac = 0.85 + t\n\n    if r_norm > mu * max(s_norm, eps):\n        return rho * fac, t, \"mul\"\n\n    if s_norm > mu * max(r_norm, eps):\n        return rho / fac, t, \"div\"\n\n    return rho, t, \"keep\"\n=======\n    t = tau(k, c, p)\n    # Compute the ratio of residuals, avoiding division by zero\n    ratio = (r_norm + eps) / (s_norm + eps)\n    \n    # Target ratio is 1, so we want ratio to be close to 1\n    # If ratio > mu, primal residual is too large -> increase rho\n    # If ratio < 1/mu, dual residual is too large -> decrease rho\n    # Use a smoother update factor based on sqrt(ratio) dampened by t\n    if ratio > mu:\n        # Increase rho by factor proportional to sqrt(ratio)\n        # But dampen with t to avoid large jumps in later iterations\n        fac = 1.0 + t * np.sqrt(ratio) / 2.0\n        return rho * fac, t, \"mul\"\n    elif ratio < 1.0 / mu:\n        # Decrease rho\n        fac = 1.0 + t * np.sqrt(1.0 / ratio) / 2.0\n        return rho / fac, t, \"div\"\n    else:\n        return rho, t, \"keep\"\n>>>>>>> REPLACE\n\nWait, but note that `t` decreases with iterations, so updates become smaller over time, which is good for stability. Also, using `sqrt(ratio)` makes updates less aggressive than using the ratio directly.\n\nHowever, we need to import numpy as np? It's already imported. Good.\n\nBut there's a potential issue: `fac` could be very large if `ratio` is very large, especially early on. Let's bound it:\nActually, `t` is at most 1.0 when k=0, and `sqrt(ratio)` could be large. Let's cap the update factor to prevent extreme changes.\n\nLet's adjust:\n\n<<<<<<< SEARCH\n    t = tau(k, c, p)\n    # Compute the ratio of residuals, avoiding division by zero\n    ratio = (r_norm + eps) / (s_norm + eps)\n    \n    # Target ratio is 1, so we want ratio to be close to 1\n    # If ratio > mu, primal residual is too large -> increase rho\n    # If ratio < 1/mu, dual residual is too large -> decrease rho\n    # Use a smoother update factor based on sqrt(ratio) dampened by t\n    if ratio > mu:\n        # Increase rho by factor proportional to sqrt(ratio)\n        # But dampen with t to avoid large jumps in later iterations\n        fac = 1.0 + t * np.sqrt(ratio) / 2.0\n        return rho * fac, t, \"mul\"\n    elif ratio < 1.0 / mu:\n        # Decrease rho\n        fac = 1.0 + t * np.sqrt(1.0 / ratio) / 2.0\n        return rho / fac, t, \"div\"\n    else:\n        return rho, t, \"keep\"\n=======\n    t = tau(k, c, p)\n    # Compute the ratio of residuals, avoiding division by zero\n    ratio = (r_norm + eps) / (s_norm + eps)\n    \n    # Target ratio is 1, so we want ratio to be close to 1\n    # If ratio > mu, primal residual is too large -> increase rho\n    # If ratio < 1/mu, dual residual is too large -> decrease rho\n    # Use a smoother update factor based on sqrt(ratio) dampened by t\n    # Cap the maximum adjustment to avoid extreme changes\n    if ratio > mu:\n        # Increase rho by factor proportional to sqrt(ratio)\n        # But dampen with t to avoid large jumps in later iterations\n        adj = np.sqrt(ratio)\n        # Bound adjustment between 1.0 and 2.0\n        adj = min(adj, 2.0)\n        fac = 1.0 + t * (adj - 1.0)\n        return rho * fac, t, \"mul\"\n    elif ratio < 1.0 / mu:\n        # Decrease rho\n        adj = np.sqrt(1.0 / ratio)\n        adj = min(adj, 2.0)\n        fac = 1.0 + t * (adj - 1.0)\n        return rho / fac, t, \"div\"\n    else:\n        return rho, t, \"keep\"\n>>>>>>> REPLACE\n\nThis bounds the adjustment factor to at most `1.0 + t*(2.0-1.0) = 1.0 + t`, which is at most 2.0 when t=1.0, and approaches 1.0 as t decreases. This prevents extreme updates.\n\nHowever, note that when `t` is small, updates are small, which is good for later iterations. Early iterations with larger `t` can make more significant adjustments.\n\nThis approach should help balance residuals more effectively and lead to faster convergence while maintaining stability.\n\nLet's also consider modifying `tau` to be more suitable for our new strategy? But the problem says we can only modify `update_rho` and helper functions. Since `tau` is a helper, we can adjust it to better suit our needs.\n\nCurrently, `tau = c / ((k+1)^p)`. For early iterations, we might want `t` to be larger to allow more aggressive updates. But the current parameters (c=1.0, p=1.2) seem fine. However, we can adjust to have `t` start at a moderate value and decrease faster.\n\nBut to keep changes minimal, we'll leave `tau` as is for now. If needed, we can adjust it in another change.\n\nLet's see if we can further improve by making the update factor depend on the iteration in a different way. However, the current approach seems promising.\n\nLet's make sure the code is deterministic and numerically safe by using `np.sqrt` which handles non-negative numbers.\n\nSince we're already importing numpy, it's fine.\n\nNow, let's summarize the changes:\n1. We use the ratio of residuals to decide updates, which is more direct than comparing each to the other scaled by mu.\n2. The update factor is based on the square root of the ratio, bounded to prevent extreme changes.\n3. The adjustment is dampened by `t`, which decreases with iterations, ensuring stability in later stages.\n\nThis should lead to better balance and faster convergence."]}}, "artifacts_json": null, "artifact_dir": null, "embedding": null}